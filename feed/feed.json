{
	"version": "https://jsonfeed.org/version/1.1",
	"title": "DK&#39;s Journey",
	"language": "en",
	"home_page_url": "https://dksjourney.com/",
	"feed_url": "https://dksjourney.com/feed/feed.json",
	"description": "Stories take the reader on a journey through the author&#39;s mind. Thanks for exploring mine!",
	"author": {
		"name": "Dennis Kennetz",
		"url": "https://dksjourney.com/about-me/"
	},
	"items": [
		{
			"id": "https://dksjourney.com/blog/lessonslearnedmovetocloud2/",
			"url": "https://dksjourney.com/blog/lessonslearnedmovetocloud2/",
			"title": "My Journey in Moving an On Premise Cluster to the Cloud 2/N - Standard vs Distributed Computing",
			"content_html": "<p>In the last edition, I spoke about the mutually exclusive problem you face when owning your own hardware, and the initial motivations for us to investigate a cloud-based solution. Read about it <a href=\"https://dksjourney.com/blog/lessonslearnedmovingtocloud1/\">here</a>!</p>\n<p>In this article, I initially planned on covering details about on-premises vs cloud high performance computing, but as I began writing I realized that there were additional topics that needed covered prior to making that jump. So today, I will focus on the differences between standard and distributed computing, which should lead into high performance computing and high throughput computing, which fall under the distributed computing domain.</p>\n<h2 id=\"what-is-standard-computing\" tabindex=\"-1\">What is standard computing? <a class=\"header-anchor\" href=\"https://dksjourney.com/blog/lessonslearnedmovetocloud2/\">#</a></h2>\n<p>Before we discuss more advanced computing models in this and later posts, let's refresh on how standard computing works. Standard computing processes data via serial computing - workloads divide into a series of steps, and then the computer executes those steps one after another on the same processor. Like humans have a <a href=\"https://senecalearning.com/en-GB/blog/long-term-memory-vs-working-memory/\">longterm memory vs working memory</a>, computers have disk storage for longterm access, and random-access memory (RAM). These are the standard locations the processor reads and writes data from / to. So if we relate a computer to a human, the processor is the brain, the disk is the brain, and RAM is the brain...</p>\n<p>Okay, I admit that wasn't helpful.</p>\n<p>Let's instead use the analogy of moving houses. We will revisit this throughout the post. When we move, consider all the tasks we need to perform:</p>\n<ol>\n<li>We must pack all our belongings into boxes</li>\n<li>We must load all the boxes onto a truck</li>\n<li>We must drive to our new home</li>\n<li>We must unload the boxes</li>\n<li>Sometimes we must repeat steps 2-4</li>\n<li>We must unpack all the boxes into the appropriate places in our new home</li>\n</ol>\n<p>In the standard, serial computing case, one person would do each of these things until they are complete and then move onto the next thing:</p>\n<img alt=\"single_person_moving_house.png\" src=\"https://github.com/drkennetz/drkennetz.github.io/blob/main/content/img/single_person_moving_house.png?raw=true\" data-hpc=\"true\" class=\"Box-sc-g0xbh4-0 kzRgrI\">\n<p>You might say, &quot;yeah, but we have parallel computing! I could do all these tasks in parallel because we have multi-processing now!&quot; And you'd be correct! This task of moving a house is a good example of a task that can be perfectly parallelized, if done correctly. When I use the term &quot;perfectly parallel&quot;, I am referring to the state where we have resources to do all of these things with a maximized amount of resources available, and each of these steps can use all of the resources to 100% efficiency.</p>\n<p>A perfectly parallel example of the above might mean that we have 5 friends available to help us move, and each wants to get done as quickly as possible because moving sucks - so we might as well get it over with. So with yourself and 5 friends, the above would look like:</p>\n<ol>\n<li>6 people pack all our belongings into boxes</li>\n<li>6 people evenly load boxes onto 6 moving trucks</li>\n<li>6 people drive to our new home</li>\n<li>6 people unload boxes</li>\n<li>6 people unpack all the boxes into the appropriate places in our new home</li>\n</ol>\n<p>Now that's what I'm talkin' about! That seems way more effective than serial computing... and it is! When tasks can be parallelized, we get the same thing done much more quickly. Now, there is also the case where only part (or parts) of the process can be parallelized, so we still get increased throughput, but it isn't quite perfect. This would likely be the case in the moving scenario, because most people wouldn't rent 6 moving trucks to move a single house. They'd rent one, and they may need to make an additional trip or two, which would slow everything down. This is an important thing to consider when designing compute processes, and even systems!</p>\n<p>In this instance, parallel computing seems like the solution to our problem. However, what if instead of moving a house, we were moving our entire 10 story office building into a new, 20 story office building because demand is booming? This would be a tall task for me and my 5 friends...</p>\n<h2 id=\"what-is-distributed-computing\" tabindex=\"-1\">What is Distributed Computing? <a class=\"header-anchor\" href=\"https://dksjourney.com/blog/lessonslearnedmovetocloud2/\">#</a></h2>\n<p>We have discussed standard computing and at the end of the last section we eluded to the fact that this may fall short of the situation where we need to do a very large job. Often enough, it isn't that standard computing couldn't handle a large task like &quot;moving an office building&quot; it would just take forever. This is often the case with data too (although oftentimes hardware limitations do in fact prevent standard computing from dealing with large volumes of data).</p>\n<p>So here we introduce distributed computing. In distributed computing, components of the system are shared among multiple networked computers or nodes. In this model, the individual components of the system interact with one another over a network (or networks) in order to achieve a common goal. There are numerous types of distributed systems, and the designs for each really should correspond to the data being processed, and the product being delivered. The pieces should be setup in a way to maximize the efficiency of the system, not any individual part.</p>\n<p>If we go back to our model of moving the office building for a moment, let's focus in on one component - loading the boxes into the truck and driving them to our new building. For the sake of our example, let's position our new office to be an hour away from our current office. Consider the following two cases:</p>\n<ol>\n<li>My friends and I load up a truck that is already on site, and I drive the boxes to the new office location, and unload them.</li>\n<li>My friends and I pack up the boxes and put them on a loading ramp, where one of many trucks will come. Each subsequent truck will arrive 30 minutes after the truck before it on a schedule, load the boxes, and drive them to the new office location, and unload them.</li>\n</ol>\n<p>So when we take a look at the next section, let's discern how distributed systems optimize for throughput, rather than latency.</p>\n<h3 id=\"latency-versus-throughput\" tabindex=\"-1\">Latency versus Throughput <a class=\"header-anchor\" href=\"https://dksjourney.com/blog/lessonslearnedmovetocloud2/\">#</a></h3>\n<p>Let's start with some definitions:</p>\n<p><strong>Latency</strong> in computer land refers to the amount of time it takes before a transfer of data <strong>begins</strong> following the <strong>instruction</strong> for its transfer. A human example of this is if I told my friend to pick up a box and put it in the truck, and it took my friend 15 seconds to move before beginning the task, the latency of that instruction would be 15 seconds.</p>\n<p><strong>Throughput</strong> simply refers to the quantity of data we can process in a given time. In our moving example, this is how many boxes we can get done in an hour.</p>\n<p>I started with these definitions so we could adequately break down the points at the end of the distributed computing intro. The first scenario where my friends and I load the boxes, drive them, and unload them optimizes for <strong>latency</strong> - the time it takes us to do this task a single time will be much faster for a single truck than it will in the case for trucks arriving on schedule.</p>\n<p>This is because in the schedule situation, we may finish getting all the boxes on the ramp 10 minutes after the previous truck leaves, meaning we would need to wait 20 minutes before the next truck even arrives. They would then need to load the truck before leaving, which would add additional time.</p>\n<p>When my friends and I do it, we can <strong>immediately</strong> put the boxes on the truck, cutting out that loading time, and drive as soon as we get all the boxes out for a truck full. We could potentially cut down 25 minutes on that single task!</p>\n<img alt=\"latency_vs_throughput.png\" src=\"https://github.com/drkennetz/drkennetz.github.io/blob/main/content/img/latency_vs_throughput.png?raw=true\" data-hpc=\"true\" class=\"Box-sc-g0xbh4-0 kzRgrI\">\n<p>We can see how this would be great if we had to do this a single time - we'd save 25 minutes! But when moving an office building, we'd likely do this tens to hundreds of times - and this is where optimizing for throughput comes in.</p>\n<p>Using the same example as above, after unloading the boxes (which takes 10 minutes) we'd have to drive back an hour just to begin the next batch. We'd then take an additional 10 minutes to load the boxes, drive an hour back, etc. etc. And we'd be working all the time!</p>\n<p>This means for 2 complete truckloads of boxes, we'd need to take:</p>\n<p>(10m to load) + (60m to drive to) + (10m to unload) + (60m to drive back) + (10m to load) + (60m to drive to) + (10m to unload)</p>\n<p>= 10+60+10+60+10+60+10 = 220 minutes</p>\n<p>Contrast that to 2 truckloads for the schedule:</p>\n<p>(10m to load) + (25m next truck + load) + (60m to drive) - but while this driving is taking place, the next truck is already in route, so do we really include the drive time? The answer is kind of... It looks a bit more like this:</p>\n<p>(30m for truck1) + (60m drive - 30m for truck2 + 10 to unload) + (60m drive - 30m for truck3 + 10 to unload)... to N trucks:</p>\n<p>= 30 + (60 - 30 + 10) + (60 - 30 + 10) = 110 minutes</p>\n<p>Basically, we offset the cost associated with a single trip by always having many trucks circulating through, which eliminates that pesky time to drive back:</p>\n<img alt=\"trucks_in_circular_route.png\" src=\"https://github.com/drkennetz/drkennetz.github.io/blob/main/content/img/trucks_in_circular_route.png?raw=true\" data-hpc=\"true\" class=\"Box-sc-g0xbh4-0 kzRgrI\">\n<p>To summarize this section, when we have many trucks in rotation, the cost of doing this a single time is more expensive - IE it has higher latency. For every single instance of a truck taking this route, the latency will ALWAYS be higher with respect to that single truck. But when we consider the cost of the whole system, the throughput will also be much higher because the latency cost of a single truck is offset by distributing the workload to many trucks over many trips.</p>\n<h2 id=\"summary\" tabindex=\"-1\">Summary <a class=\"header-anchor\" href=\"https://dksjourney.com/blog/lessonslearnedmovetocloud2/\">#</a></h2>\n<p>This is how distributed computing systems work. The latency of a single operation between two separate components of the system <strong>will</strong> be higher than in standard computing because the operation will occur over a network. In standard computing, the two operations will occur on the same machine, so the latency is just the process communicating with another part of the same machine.</p>\n<p>Standard computing will work fine up until a volume threshold - your computer can handle a fairly large number of tasks. But when we scale to thousands, then millions, then billions of operations, the throughput optimized distributed system wins out.</p>\n<p>In the next post, I'll talk about two different types of distributed systems used for performing analyses on large quantities of data - High Performance Computing (HPC) and High Throughput Computing (HTC). I'll touch on the advantages and disadvantages of each, and when they are appropriate to use. After painting this picture, I'll (hopefully) arrive at the decision which led our team to the cloud in an effort to solve our mutually exclusive problem (see last post for details).</p>\n<p>As always, thanks for reading!</p>\n<p>Dennis</p>\n",
			"date_published": "2023-07-03T00:00:00Z"
		}
		,
		{
			"id": "https://dksjourney.com/blog/lessonslearnedmovingtocloud1/",
			"url": "https://dksjourney.com/blog/lessonslearnedmovingtocloud1/",
			"title": "My Journey in Moving an On Premise Cluster to the Cloud 1/N",
			"content_html": "<p>Wordy enough title, am I right?</p>\n<p>Over the past two years, I have had the opportunity to design and develop all the infrastructure and tooling required to transfer an on-premise data center\nto a distributed computing monster in Azure, utilizing their CycleCloud technology. Journey with me as I detail the blood, sweat, and tears that go into\nleading a project like this. I'll go through why this was even considered in the first place, infrastructure requirements for each, getting CEO/CIO approval, building out the real thing with testing, into initial production and beyond. This was a real test of grit and character, and I'm excited to share this story with you all. I don't know how many parts this will contain, but I can't possibly fit it all into one - so I've deemed this 1/N. I hope it is captivating enough to follow along with the story!</p>\n<h2 id=\"why-move-the-data-center-anyway\" tabindex=\"-1\">Why move the data center, anyway? <a class=\"header-anchor\" href=\"https://dksjourney.com/blog/lessonslearnedmovingtocloud1/\">#</a></h2>\n<p>Good question. I'll start by saying this next part isn't the responsibility of my team, but these are things I learned along the way.</p>\n<p>When you purchase big compute nodes, you buy them from some vendor at a large scale. When you purchase them, you can also purchase something called a\nsupport contract, where the vendor will help you troubleshoot any hardware / software failures directly related to their technology. However, for a given\nset of hardware, that support contract has a lifetime based on the expected lifetime of the resource. For example, if you purchased nodes that were expected to last\ntwo years before burning out, the support contract would likely last two years. Our previous nodes were under contract for five years, and the five years was\nabout to run out. Basically, this hardware was end of life which requires a complete refresh.</p>\n<p>A refresh means that the institution would need to buy a whole new set of compute nodes, and these would need to be justified by some serious metrics to support the cores\nand RAM that were to be purchased in the refresh. We were at the refresh, and my manager at the time needed to make a case for the compute we needed for our production\nworkload. So he gathered up all the numbers, and found a very interesting trend...</p>\n<h3 id=\"a-mutually-exclusive-problem\" tabindex=\"-1\">A mutually exclusive problem <a class=\"header-anchor\" href=\"https://dksjourney.com/blog/lessonslearnedmovingtocloud1/\">#</a></h3>\n<p>Imagine you're a chef and you make pizzas for a living. Like any good pizza chef, you only cook your pizzas in a pizza oven. Your pizzeria is open 6 days a week, and it is absolutely poppin' - but only on Fridays and Saturdays. On Fridays and Saturdays, you have more customers than you do pizza ovens, often leading to huge wait times for your delicious pizza. You realize with all your weekend customers you can afford to buy some new pizza ovens! Good on you. This allows you to handle those peak times a little better, but what about those other pesky days where you aren't that busy? Your pizza ovens sit idle most of the day... and this isn't great. You wish there was some way you could have all the pizza ovens you need on the weekend, but during the week you could rent those pizza ovens out to other pizza chefs who may be in need of them.</p>\n<img alt=\"pizza_ovens_by_time.png\" src=\"https://github.com/drkennetz/drkennetz.github.io/blob/main/content/img/pizza_ovens_by_time.png?raw=true\" data-hpc=\"true\" class=\"Box-sc-g0xbh4-0 kzRgrI\">\n<p>This fun little pizza oven graph shows two things:</p>\n<ol>\n<li>Pizza chef does not have enough ovens to deal with peak demand.</li>\n<li>For a good portion of time, the pizza ovens are barely being used.</li>\n</ol>\n<p>So this leads to the mutually exclusive problem we faced with compute in our data center. We would receive the majority of our data heading into the weekend. We only had 1300 cores available, and sometimes these data peaks would require nearly 10X that, so we'd have a tremendous amount of pending time for a lot of those data processing jobs. However, after that peak was processed, the 1300 cores largely remained idle - they weren't doing any work. The dilemma my manager found was this:</p>\n<ol>\n<li>The cores over the time average of a year were being utilized about 7% of the time - IE 93% of the time they remained idle with no data processing jobs.</li>\n<li>During peak times, jobs would pend for several hours, and the cores would be occupied at nearly 10X their capacity (13,000 jobs for 1,300 cores)</li>\n<li>We could not solve both the efficiency and capacity problems on hardware we owned.</li>\n</ol>\n<p>We would either need to buy more hardware to meet peak demand which would decrease utilization leading to more waste (the more likely scenario), or we would need to buy less cores to increase percent utilization decreasing waste but increasing pending time - ultimately decreasing throughput (no one would go for this from a data processing perspective).</p>\n<p>So this led us to investigate the third option, which was to move to an elastic computing model in the cloud. This model would have a different infrastructure than an on-prem data center model and would be faced with its own challenges, but could potentially solve both problems.</p>\n<p>In the next post, before I dive into the solution to the problem stated above, I feel it appropriate to talk about the differences between two computing models - standard vs distributed computing. When we have a grasp on that, I'll touch on two different types of distributed computing: High Performance Computing (HPC) vs High Throughput Computing (HTC) before finally arriving at the solution for our mutually exclusive problem. I hope you are able to learn a lot from these posts as you follow along with my journey.</p>\n<p>Thanks for reading,</p>\n<p>Dennis</p>\n",
			"date_published": "2023-06-25T00:00:00Z"
		}
		,
		{
			"id": "https://dksjourney.com/blog/journeyintodevelopment/",
			"url": "https://dksjourney.com/blog/journeyintodevelopment/",
			"title": "My Journey through Life into Software Development",
			"content_html": "<p>In this post I plan to touch on my strange journey into and through software development. This is a long one, so buckle up. There have been some strange turns along the way, and I've met some amazing people that have taught me some amazing things. I also want to highlight that the only path into becoming an engineer is not through a Computer Science degree. I hope to encourage some potential &quot;non-traditional&quot; devlopers. There are many ways to get where you want to go. I could've never dreamed of mine, but it worked out in an amazing way.</p>\n<h2 id=\"how-did-it-all-begin\" tabindex=\"-1\">How did it all begin? <a class=\"header-anchor\" href=\"https://dksjourney.com/blog/journeyintodevelopment/\">#</a></h2>\n<p>School always came pretty easy to me. I didn't have to work super hard to get good grades, and my brain has always seemed to be particularly well-wired for math and science. These two fields were almost just intuitive to me. Because of that, I actually spent very little time focusing on school. I was a competitive soccer player, and my goal coming out of high school was to play division 1 soccer. I traveled almost every weekend, and I loved the game. Fast-forwarding to today, I have realized that competitive sports prepared me for a lot more of the real world challenges I face on a daily basis than school ever could have (and maybe this is content for a different post).</p>\n<p>As I was approaching the end of high school, I was able to turn my dream into a reality - I had an offer to play division 1 soccer at a small school in North Carolina called Gardner-Webb University. As I prepared to go, I also had to think about what I wanted to study while in school. A pretty close family friend was a dentist - and that seemed like a pretty chill job - so, impulsively (I'm rather impulsive), I decided that would be my path. Gardner-Webb didn't have a &quot;pre-dentistry&quot; path, so like any good pre-health kid, I chose Biology as a major. You meet many such kids in your studies in Biology, and I'd say about 10% actually end up doing what they set out to do - entirely speculative BTW. School went fine, and soccer went great - but I found out that first semester that I was a homebody. I transferred back to the University of Memphis the following spring, and walked on to the UofM soccer team after a spring tryout.</p>\n<p>Fast-forward to about Junior year - I tore my ACL (a pretty significant injury for a soccer player) and was required to red-shirt a season as the total recovery time between surgery and rehab can take 6 months on the low end, to a year on the high end - and you never <em>fully</em> recover from such an injury. This is actually an important piece of the story because it was during this year that I realized:</p>\n<ol>\n<li>I did not want to become a dentist.</li>\n<li>I got an extra year of undergrad for free.</li>\n</ol>\n<p>Yes, I decided dentistry was not for me. So what next? Well obviously Medical school, right? The pre-requisite classes up to that point were the same for both curriculums, and I was a bit more fascinated by being a Medical Doctor than a Dentist (spoiler alert, I came to realize that neither of these things were going to work out). At this point, I was in my upper division science classes, and I was making really good grades - all seemed to be flowing smoothly. I had a few 4.0 semesters in a row, soccer was going well, and I was flying high.</p>\n<h2 id=\"the-turning-point\" tabindex=\"-1\">The turning point <a class=\"header-anchor\" href=\"https://dksjourney.com/blog/journeyintodevelopment/\">#</a></h2>\n<p>I get to my fifth year of undergrad having already completed my biology degree, I began doing 2 things:</p>\n<ol>\n<li>Shadowing some doctors</li>\n<li>Finishing a Chemistry Degree</li>\n</ol>\n<p>I found out that I needed like 9 more hours based on the courses I had already completed to finish a chemistry degree, and so I planned out my courses over the next two semesters and realized it could be done. So I embark on my final year. Bad thing one happens: I got complacent with my spot on the team in soccer, and I was replaced (by a freshman). I figured having gotten good minutes as a starter all of my previous years (even winning our &quot;teammate of the year award&quot; as a fourth year), I was a shoe-in to just be &quot;given&quot; a starting position as a fifth year senior. Boy was I wrong. I barely got any playing time all year, and this made me bitter towards soccer. Did I agree with the decision at the time? No. Did it teach me things? At the time, no. Today - many things. We have to work for every inch of what we want. As soon as we take our foot off the gas, someone else steps in.</p>\n<p>And hey - there are seasons in life where that is okay. I'm a husband and a dad of two beautiful kids right now, and I don't have the time I did 4 or 5 years ago. Have I stopped pushing myself? Not at all, I seek to learn all the time. But I also enjoy my family more than I do my computer (shocker), so I spend time with them instead.</p>\n<p>Okay, back to the story. Soccer ends, and the year <strong>sucked</strong>. It was not the way I would've chosen to end my soccer career, but that's what God handed me, and He used it well to shape my future. In my very last semester of college, I took physical chemistry which is a blend between physics and chemistry - or more specifically - the physics of chemistry. I loved it. It absolutely fascinated me, and because of this I excelled at it. But I was still going to become a doctor.</p>\n<p>I graduated and began my studies for the MCAT. I took the kaplan course and everything. Except, I hated every minute of trying to study for that dreadful exam. So much so, that instead of studying, a lot of times I played video games. This wasn't new - I did this all through undergrad too, and I was okay... I'd be okay again, right? Wrong. I took the MCAT at the end of summer and did okay. Not bad, not great, just okay. I applied for a medical school here in town, and got passed onto the second round, then got invited to do an interview! This is a big step for anybody familiar with the process.</p>\n<p>The interview was setup for February of the following year, so I had plenty of prep time - it was still fall. During that waiting period, the graduate school recruiter in the Chemistry department reached out to me about doing, well, graduate school. I was a good candidate as I had a good GPA. One thing that is cool about graduate school in the sciences is that it is free if you do research. I spoke with the Medical School Dean about this opportunity, and he encouraged me to pursue it. His words, &quot;Medical school isn't going anywhere. If you finish a Master's or a PhD and decide you still want to become a Medical Doctor, we will still need medical doctors, and you'll be a better candidate. You can't beat free education. If you have any interest at all, go do it.&quot; This is one of the few times in my life where I actually listened to the wisdom of those that had come before me.</p>\n<p>I did my GRE and started Graduate School in the spring of 2015. I actually already knew what area I wanted to study - physical chemistry. I did some interviews, and found a computational lab that did theoretical work describing real systems using computer simulations. Mind you, I knew <em>literally</em> nothign about programming at this point in my life. But I liked computers, I liked video games, how different could it be?</p>\n<h2 id=\"my-first-programming-experience\" tabindex=\"-1\">My first programming experience <a class=\"header-anchor\" href=\"https://dksjourney.com/blog/journeyintodevelopment/\">#</a></h2>\n<p>Here comes the part we've all been waiting for. My lab analyzed these systems we call &quot;Molecular Dynamics Simulations&quot;. We put some molecules in a constrained system, we add some different forces to the system, and we let it play out over time. When it's all done, we have a big file full of molecular coordinates at different time points, and we analyze those things to make sense of the system. Surely we just do this in Excel right? We can make pretty pictures, put things in columns, and even write some nifty equations. Yeah, I was naive. This is where I learned about python, and specifically the numpy, pandas, matplotlib ecosystem. I kind of became familiar with our scripts, and learned how to write a few functions to do the math calculations we needed to make sense of our data. I learned that we could store our data in these things called <code>np.array</code>'s, and we could iterate over those using these <code>for</code> loops, and the computer could really do all this processing pretty darn quickly. It was cool.</p>\n<p>During this time, I was a pretty bad student. I mean, I did well in school, but I was having a really hard time balancing research (which is a full time job), teaching 2 labs, and studying for courses. I ended up having these super weird hours, and my sleep schedule sort of flip-flopped. I was pretty unhealthy, but I was also doing really well - until I wasn't. Mentally, I was stressed out. PhD's require grit. Like real grit. And if you don't love what you're doing, it can be hard to push on. In reality, I was immature. Having never had to really work hard at school, it kind of knocked me back a bit to have to study and work at the same time. When the time came to decide between pushing on to a PhD or &quot;master out&quot;, I &quot;mastered out&quot;. (This just means stopping at a Master of Science). My grad school experience went well from an accomplishment standpoint. <a href=\"https://pubs.acs.org/doi/10.1021/acs.jpcb.6b10775\">I published a paper with my group</a>, and was well on my way to publishing a second, with research lined up to finish a third. But I couldn't bear it. I was done with school, like, SO done. I finished my masters and got the heck out of dodge. One thing I took away from all that hard work - I did like programming, just not in that environment.</p>\n<h2 id=\"my-first-professional-experience\" tabindex=\"-1\">My first professional experience <a class=\"header-anchor\" href=\"https://dksjourney.com/blog/journeyintodevelopment/\">#</a></h2>\n<p>Even with a MS in Chemistry, getting your first job is hard. I applied to 100+ jobs in my first month out of school, and initially I heard back from 1, which was a sales gig selling Direct TV in Walmart. That stuff is brutal. 12 hour days, cold stopping people to ask if they want your stuff, pestering, bothering, not my personality. I lasted 2 weeks. I'm married - no kids - by this point and my wife and I decided this just wasn't going to work. We could swing it a few months based on what we had saved up before I got my first job. Fortunately, this job had a trial period to see if you'd be a good fit (high turnover job), and at the end of that trial I told him I wouldn't be continuing. He wished me the best.\nThis is one of those providential moments in life. When I got off the phone, my wife and I said a prayer. Immediately when the prayer ended, and I mean immediately - like within 2 seconds, I heard my phone ring. I picked it up, and it was the Director of the Genomics lab at St. Jude - one of the jobs I had applied for. He was interested in me! We setup a phone interview for 10 days out, had an onsite at the beginning of July, and I started as a research technologist July 31 with $12 in our bank account. Funny story because of how well this job worked out for me, but I was actually the 3rd choice out of 5 final candidates selected for the role. The first was hired, the second got a different job so she turned this one down, and then there was me. I only got hired because another rec was opened up directly under John, my Director and soon to be boss. He hired me <em>because I had some linux experience</em>. Haha.</p>\n<p>I had no idea how valuable <em>some linux experience</em> would be for my future.</p>\n<p>When I started, there was this process to kick off a process called &quot;demultiplexing&quot; of sequencing runs. It was done on a cluster because it used a good amount of memory and required many cores to be done in a reasonable amount of time. The whole process looked like:</p>\n<ol>\n<li>Edit a samplesheet on a desktop.</li>\n<li>Put the samplesheet in the sequencing run directory on the cluster.</li>\n<li>Kick off a script that took the path to the sequencing run as the first argument and wait for it to finish.</li>\n<li>When it finished, notify the software engineering group.</li>\n</ol>\n<p>That process is actually kind of challenging for someone who has no experience on the command line. Command lines feel weird when you're not used to linux. You can't click, you can't open files without a text editor, and you have to execute little commands to actually see where you are on the filesystem. This is weird when you're not used to it. John figured it was my first week and it would take me some time to get up to speed with that process, so he allotted me something like 6 hours to do that for the few sequencing runs that we'd process a week. The first time a sequencing run came through, it took me about 15 minutes to work out the whole process. The next time, I thought about automating it. The final time, I automated it. Within the first week, I had taken 6 hours off my assigned work because I was somewhat competent on the command line.</p>\n<p>John liked that. He didn't assign new work for that time, and I think that was absolutely pivotal to my rapid expansion into the bioinformatics space. Instead of new work, he gave me research papers to read, common bioinformatics programs to learn, and stuff to study to really understand the process of sequencing DNA and RNA. Not to boast, but I took off. I was fascinated by the tech, the science, and the programming to process the data. I spent time reading, and quickly realized programming (that thing I kind of liked) could be a game changer for me in this field. I will never forget the two courses I took to learn how to be better at python:</p>\n<ol>\n<li><a href=\"https://www.coursera.org/learn/python-data-analysis\">Introduction to Data Science in Python</a></li>\n<li><a href=\"https://www.coursera.org/learn/python-genomics\">Python for Genomic Data Science</a></li>\n</ol>\n<p>These got me running. Pretty soon, John had real confidence in me to get things done on the computer. My work quickly shifted from &quot;learning wet lab&quot; to &quot;doing dry lab&quot;. In my relatively short tenure in the lab, I developed mapping pipelines, QC pipelines, and even wrote our own primer multiplexing algorithm called <a href=\"https://github.com/stjude/PrimerTK\">PrimerTK</a>. Looking back on it now, it is pretty bad code - but it does a pretty good thing. And it allowed me to apply what I had learned in python to a real project which I would eventually publish.</p>\n<p>During this time, some of the Senior Software Engineers in the broader department had taken notice of me too. They liked the work I did, they answered my silly questions, and how passionate I was about learning new things and diving into projects. As a research technologist, I was actually even invited into some of the software engineering group meetings, and some of the project planning meetings when they didn't interfere with my lab work. The interest they took in me and the willingness to help me is something I will never forget, and I will always do my best to pay forward.</p>\n<p>One project in particular came up around these new-ish things in bioinformatics called &quot;Workflow Languages&quot;. A workflow is any well-defined series of steps required to get input data to some form of output data that you want. Bioinformatics is characterized by workflows. You generally have some raw data, and you have to run some series of executable programs (distinct) to get your final resultant data. This project involved seeing what it would take to convert some of the software engineering group &quot;workflows&quot; into these &quot;workflow languages&quot;. The two we finally decided between were called the <a href=\"https://www.commonwl.org/\">Common Workflow Language</a> and the <a href=\"https://openwdl.org/\">Workflow Description Language</a>. This was my time to shine, and I shined bright. I became the resident expert in both of these, learning them and implementing some of our non-trivial bioinformatics workflows fairly quickly. There was a lot of complexity in this, and looking back, I did a good job.</p>\n<p>Little did I know, in combination with my python skills and my bioinformatics knowledge, this would be the skill that got me hired to my first software engineering position. One of the senior engineers that had worked with me a good bit was promoted to an Engineering Manager, and he was given some positions to hire. Some of the key skills for one of his roles were python and workflow language knowledge, as well as a background in bioinformatics tooling (it's almost like the position was created with me in mind :D). He approached me one day after work and asked if I was interested in Software Engineering. When I said yes, he spoke with John about the role (at this point, I didn't know about the role). When he spoke with John, John spoke with me. John was an outstanding boss, and truly wanted to see me succeed - and be happy. He sat me down and asked about my future plans. He asked if I liked wet lab, or if I wanted to be a software engineer. When I expressed interest in the engineering side, he told me about the role and the opportunity. He even went to bat for me to get me a good salary. John is an amazing man.</p>\n<p>Things moved quickly, and I took the role - my first software engineering role.</p>\n<h2 id=\"finishing-thoughts\" tabindex=\"-1\">Finishing thoughts <a class=\"header-anchor\" href=\"https://dksjourney.com/blog/journeyintodevelopment/\">#</a></h2>\n<p>I have had the opportunity to work on some amazing things as a software engineer, and I have loved it. However, the road here was not easy, and it certainly wasn't quick. I had many bumps and bruises along the way. There were times I wanted to give up, times I thought it would never work out, and times that were also great when I finally grasped that new concept. I wanted to include this because if you are reading this and you feel a little lost in life, keep at it. I spent years in the wilderness trying to find my way out. The path was murky, and often times I couldn't see past the next few trees in front of me. But through grit, help and support from loved ones, and a tiny bit of faith, I was able to keep moving forward. As I kept going, the road became a bit clearer, the path less and less murky, until finally it was clear. Whether you're trying to break into your first developer role, or just lost in the woods somewhere, keep at it. It gets better as long as you keep moving forward!</p>\n<p>Take care,</p>\n<p>Dennis</p>\n",
			"date_published": "2023-06-16T00:00:00Z"
		}
		
	]
}
